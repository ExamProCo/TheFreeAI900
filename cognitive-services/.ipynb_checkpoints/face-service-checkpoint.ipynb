{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install azure-cognitiveservices-vision-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "    \n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ff5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congitive Key and Endpoint Configuration\n",
    "my_key      = 'MY_COGNITIVE_KEY'\n",
    "my_endpoint = 'MY_COGNITIVE_ENDPOINT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2367243",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = FaceClient(my_endpoint, CognitiveServicesCredentials(my_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our image into a stremable format\n",
    "path = os.path.join('assets', 'data.jpg')\n",
    "stream = open(path, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb840eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.face.detect_with_stream(stream)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac34482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show face\n",
    "img = Image.open(path)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "num_faces = len(results)\n",
    "prediction = ' (' + str(num_faces) + ' faces detected)'\n",
    "# Draw a rectangle around each detected face\n",
    "for face in results:\n",
    "    r = face.face_rectangle\n",
    "    bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(bounding_box, outline='magenta', width=1)\n",
    "    plt.annotate(\"Face ID:\" + face.face_id,(r.left, r.top + r.height + 15), backgroundcolor='white')\n",
    "#a = fig.add_subplot(1,1,1)\n",
    "fig.suptitle(prediction)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f88111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our image into a stremable format\n",
    "path = os.path.join('assets', 'data-large.jpg')\n",
    "stream = open(path, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show face attributes\n",
    "attributes = ['age', 'emotion','makeup','gender']\n",
    "detected_faces = client.face.detect_with_stream(image=stream, return_face_attributes=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(path)\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "num_faces = len(detected_faces)\n",
    "prediction = ' (' + str(num_faces) + ' faces detected)'\n",
    "\n",
    "for face in detected_faces:\n",
    "    r = face.face_rectangle\n",
    "    bounding_box = ((r.left, r.top), (r.left + r.width, r.top + r.height))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.rectangle(bounding_box, outline='magenta', width=1)\n",
    "\n",
    "    # Annotate with face attributes\n",
    "    detected_attributes = face.face_attributes.as_dict()\n",
    "    age = 'age unknown' if 'age' not in detected_attributes.keys() else int(detected_attributes['age'])\n",
    "    annotations = 'Person aged approximately {}'.format(age)\n",
    "    annotations += '\\nGender: {}'.format(detected_attributes['gender'])\n",
    "    annotations += '\\nMakeup: {}'.format(detected_attributes['makeup'])\n",
    "    txt_lines = 1\n",
    "    if 'emotion' in detected_attributes.keys():\n",
    "        for emotion_name in detected_attributes['emotion']:\n",
    "            txt_lines += 1\n",
    "            annotations += '\\n - {}: {}'.format(emotion_name, detected_attributes['emotion'][emotion_name])\n",
    "    plt.annotate(annotations,((r.left + r.width), (r.top + r.height + (txt_lines * 12))), backgroundcolor='white')\n",
    "\n",
    "\n",
    "fig.suptitle(prediction)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db5aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
